{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd721ff1-fb00-44a1-ba0a-656e5fe36020",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import Flatten, concatenate\n",
    "from tensorflow.keras.layers import RandomFlip, RandomRotation, RandomTranslation\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.layers import Dot, Add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc90a55b-6675-42e8-9fdf-134bb8d898bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77e430a3-15d6-46cf-b625-dde690235e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb571f42-837c-435e-ad75-ef75b57007d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import SGD, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c171e2fd-6b40-4f5b-887a-94ceb7c56127",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fc5f56b-fc18-48d8-ad44-323c437cddbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_hf5(data_path, data_name, get_name):\n",
    "    hf = h5py.File(os.path.join(data_path, data_name), mode=\"r\")\n",
    "    data = hf.get(get_name)\n",
    "    stack = np.array(data)\n",
    "    hf.close()\n",
    "    return stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "384e63b1-b121-4fa3-b6c2-b692903b700f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code from https://github.com/tgrippa/Partimap/blob/cd72ecd2f3e32794eb3e0ee70399de1811b5399e/Notebooks/Data_preprocessing.ipynb\n",
    "def normalise_01(image_data):\n",
    "    stack = image_data\n",
    "    image_data -= np.min(stack, axis=0)\n",
    "    image_data /= (np.max(stack, axis=0) - np.min(stack, axis=0))\n",
    "    return image_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b66a8b85-5b7a-4bd3-8087-2490b4b59cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardise(image_data):\n",
    "    image_data -= np.mean(image_data, axis=0, dtype=np.float32)\n",
    "    image_data /= np.std(image_data, axis=0, dtype=np.float32)\n",
    "    return image_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0bce6b3f-44b8-43c9-9ddc-8d055aa25bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def circle(train_lulc,test_lulc,title) :\n",
    "    train_slum = (train_lulc==1).sum()\n",
    "    train_noslum = (train_lulc==0).sum()\n",
    "    test_slum = (test_lulc==1).sum()\n",
    "    test_noslum = (test_lulc==0).sum()\n",
    "    \n",
    "    RESTRAIN = [train_slum,train_noslum]\n",
    "    RESTEST = [test_slum,test_noslum]\n",
    "    RESLABEL1 = ['Training slums', 'Training no slums' ]\n",
    "    RESLABEL2 = ['Test slums', 'Test no slums']\n",
    "    \n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    ax1.pie(RESTRAIN, labels=RESLABEL1, autopct='%1.1f%%',\n",
    "        shadow=True, startangle=90)\n",
    "    ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "    ax2.pie(RESTEST, labels=RESLABEL2, autopct='%1.1f%%',\n",
    "        shadow=True, startangle=90)\n",
    "    ax2.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "    \n",
    "    \n",
    "    plt.title(title)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d640a905-e845-4614-b679-419f45650f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from osgeo import gdal\n",
    "import numpy as np\n",
    "\n",
    "#code inspired from https://github.com/tgrippa/Partimap/blob/cd72ecd2f3e32794eb3e0ee70399de1811b5399e/Notebooks/Data_preprocessing.ipynb\n",
    "def stackage(list_raster):\n",
    "    stack = []\n",
    "    for patch in list_raster :\n",
    "        pat = gdal.Open(patch)#.decode('ascii')\n",
    "        data_rast = pat.ReadAsArray().astype(np.float32)\n",
    "        data_rast = np.transpose(data_rast,(1,2,0))\n",
    "        stack.append(data_rast)\n",
    "\n",
    "    stack= np.array(stack)\n",
    "    \n",
    "    return stack\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad53a1ba-bf36-4788-8b3e-c40deee6e2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assimilation(patch_id_train, patch_id_test, path_raster):\n",
    "    train_path = [None] * 10\n",
    "    test_path = [None] * 10\n",
    "\n",
    "    patch_id_train_re = patch_id_train.reshape(1,patch_id_train.size)\n",
    "    patch_id_train_list = patch_id_train_re.tolist()[0]\n",
    "    patch_id_test_re = patch_id_test.reshape(1,patch_id_test.size)\n",
    "    patch_id_test_list = patch_id_test_re.tolist()[0]\n",
    "    \n",
    "    train_path = [None] * len(patch_id_train)\n",
    "    test_path = [None] * len(patch_id_test)\n",
    "    \n",
    "    for path in path_raster : #récupération des id des images\n",
    "        image = path.split(b'/')[-1]\n",
    "        image_id = int(image.split(b'_')[-2])\n",
    "        if image_id in patch_id_train_list :\n",
    "            index = patch_id_train_list.index(image_id)\n",
    "            train_path[index] = path\n",
    "        elif image_id in patch_id_test_list :\n",
    "            index = patch_id_test_list.index(image_id)\n",
    "            test_path[index] = path\n",
    "\n",
    "    #stack = stackage(train_path)\n",
    "\n",
    "    stack_train = stackage(train_path)\n",
    "    #stack = stackage(test_path)\n",
    "    stack_test = stackage(test_path)\n",
    "\n",
    "    \n",
    "    plt.rcParams[\"figure.facecolor\"] = 'w'\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    for i in range(3):\n",
    "        rd_img = np.random.randint(1,500)\n",
    "        ax = plt.subplot(1, 3, i + 1)\n",
    "        plt.imshow(Norma_Xpercentile(stack_test[rd_img,:,:,:]))\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\"Random indx: %s\\nID: %s\\nlulc: %0.3f\" %(rd_img,patch_id_test[rd_img],patch_lulc_test[rd_img]))\n",
    "        print(stack_test[rd_img,0,0,:])\n",
    "        print(test_path[rd_img])\n",
    "        print(patch_id_test[rd_img])\n",
    "    #plt.subplots_adjust(hspace=0.001)\n",
    "    plt.tight_layout() \n",
    "    \n",
    "    stack_train = normalise_01(stack_train)\n",
    "    stack_test = normalise_01(stack_test)\n",
    "\n",
    "\n",
    "    return stack_train, stack_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d310dac8-458a-4a8c-a353-2ab0e199667f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_csv(predict, patch_id_raster, name):\n",
    "    import csv\n",
    "\n",
    "    predict = predict.reshape(1,predict.size).tolist()[0]\n",
    "\n",
    "    header = ['prob_slum','id']\n",
    "    cont = []\n",
    "    cont.append(header)\n",
    "    for x,y in zip(predict,patch_id_raster):\n",
    "        current_row = [x,y[0]]\n",
    "        cont.append(current_row)\n",
    "\n",
    "    cont\n",
    "\n",
    "    with open(str(name)+'.csv','w') as file :\n",
    "        write = csv.writer(file)\n",
    "        write.writerows(cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6eeb2b25-770b-4816-a03a-f3749b4370d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape(list_input):\n",
    "    L = np.array(list_input)\n",
    "    L = L.reshape(L.size,-1)\n",
    "    \n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d85b6942-6644-42db-bcde-ec62b3adc6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import os, sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ecc923d-de05-4607-b342-683a1ed8b8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21085247-3e10-409c-9303-affe4af533f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add local module to the path\n",
    "src = os.path.abspath('SRC/')\n",
    "if src not in sys.path:\n",
    "    sys.path.append(src)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a6c52fb-1048-438e-ab98-16950c7fba32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import functions for processing time information\n",
    "from processing_time import start_processing, print_processing_time\n",
    "# Import function that checks and creates folder\n",
    "from mkdir import check_create_dir\n",
    "# Import functions for plots\n",
    "from plots import plot_loss, plot_pred_test, plot_pred_train, plot_loss_multirun\n",
    "# Import functions for metrics to monitoring accuracy\n",
    "from metrics import coeff_determination\n",
    "# Import functions for display\n",
    "from display import Norma_Xpercentile\n",
    "# Import functions for exportation of results\n",
    "from export_results import save_predictions, write_run_metrics_file\n",
    "# Import LRFinder function\n",
    "from LRFinder import LRFinder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f702ae-a6ee-4aeb-894e-9ef6d0b53e3f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Importing and pre-processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ca9cf54-c28e-46c0-90c5-669dbdff018a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(238890.0, 288890.0, 9846380.0, 9871680.0)\n",
      "(238890.0, 288890.0, 9846380.0, 9871680.0)\n"
     ]
    }
   ],
   "source": [
    "from chunckage import chunckage\n",
    "\n",
    "S1_patch_path, S1_patch_id, S1_patch_lulc = chunckage(\"Images/big_raster/\",\"ALLS1\",\".tif\",'Images/big_sample/nairobi_samples_binary/','nairobi_samples_binary.gpkg','tr_ALLS1','cat','label',1)\n",
    "S2_patch_path, S2_patch_id, S2_patch_lulc = chunckage(\"Images/big_raster/\",\"BNIR\",\".tif\",'Images/big_sample/nairobi_samples_binary/','nairobi_samples_binary.gpkg','tr_BNIR','cat','label',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9829b79-9fea-44ec-a992-f610b73a6a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(238890.0, 289090.0, 9846380.0, 9871680.0)\n",
      "(238890.0, 289090.0, 9846380.0, 9871680.0)\n"
     ]
    }
   ],
   "source": [
    "S1_grid_path, S1_grid_id = chunckage(\"Images/big_raster/\",\"ALLS1\",\".tif\",'Images/big_grid/','grid.gpkg','g_ALLS1','id','lulc',0)\n",
    "S2_grid_path, S2_grid_id = chunckage(\"Images/big_raster/\",\"BNIR\",\".tif\",'Images/big_grid/','grid.gpkg','g_BNIR','id','lulc',0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "955f131d-1d75-43c6-9002-64a5d31c4be8",
   "metadata": {},
   "outputs": [
    {
     "ename": "BlockingIOError",
     "evalue": "[Errno 11] Unable to open file (unable to lock file, errno = 11, error message = 'Resource temporarily unavailable')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBlockingIOError\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m data_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mImages/big_raster/tr_BNIR_patch/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mh5py\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtr_Ymodel_S1BNIR.hdf5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ma\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      3\u001b[0m     f[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid_sar\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m S1_patch_id               \n\u001b[1;32m      4\u001b[0m     f[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid_rgb\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m S2_patch_id                 \n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py:507\u001b[0m, in \u001b[0;36mFile.__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, **kwds)\u001b[0m\n\u001b[1;32m    502\u001b[0m     fapl \u001b[38;5;241m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[1;32m    503\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    504\u001b[0m     fcpl \u001b[38;5;241m=\u001b[39m make_fcpl(track_order\u001b[38;5;241m=\u001b[39mtrack_order, fs_strategy\u001b[38;5;241m=\u001b[39mfs_strategy,\n\u001b[1;32m    505\u001b[0m                      fs_persist\u001b[38;5;241m=\u001b[39mfs_persist, fs_threshold\u001b[38;5;241m=\u001b[39mfs_threshold,\n\u001b[1;32m    506\u001b[0m                      fs_page_size\u001b[38;5;241m=\u001b[39mfs_page_size)\n\u001b[0;32m--> 507\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mmake_fid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muserblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfcpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswmr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mswmr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_libver \u001b[38;5;241m=\u001b[39m libver\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py:232\u001b[0m, in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# Open in append mode (read/write).\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;66;03m# If that fails, create a new file only if it won't clobber an\u001b[39;00m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;66;03m# existing one (ACC_EXCL)\u001b[39;00m\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 232\u001b[0m         fid \u001b[38;5;241m=\u001b[39m \u001b[43mh5f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh5f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mACC_RDWR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;66;03m# Not all drivers raise FileNotFoundError (commented those that do not)\u001b[39;00m\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m fapl\u001b[38;5;241m.\u001b[39mget_driver() \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m    235\u001b[0m         h5fd\u001b[38;5;241m.\u001b[39mSEC2,\n\u001b[1;32m    236\u001b[0m         \u001b[38;5;66;03m# h5fd.STDIO,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    243\u001b[0m         h5fd\u001b[38;5;241m.\u001b[39mROS3D \u001b[38;5;28;01mif\u001b[39;00m ros3 \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    244\u001b[0m     ) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5f.pyx:106\u001b[0m, in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mBlockingIOError\u001b[0m: [Errno 11] Unable to open file (unable to lock file, errno = 11, error message = 'Resource temporarily unavailable')"
     ]
    }
   ],
   "source": [
    "data_path = 'Images/big_raster/tr_BNIR_patch/'\n",
    "with h5py.File(os.path.join(data_path,\"tr_Ymodel_S1BNIR.hdf5\"), mode=\"a\") as f:\n",
    "    f[\"id_sar\"] = S1_patch_id               \n",
    "    f[\"id_rgb\"] = S2_patch_id                 \n",
    "    f[\"lulc_sar\"] = S1_patch_lulc          \n",
    "    f[\"lulc_rgb\"] = S2_patch_lulc       \n",
    "    f[\"patch_sar_path\"] = S1_patch_path  \n",
    "    f[\"patch_rgb_path\"] = S2_patch_path\n",
    "\n",
    "print(\"Data exported in %s\"%(data_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c46fa68-894d-4da2-890a-559c0fa9bd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'Images/big_raster/g_BNIR_patch/'\n",
    "with h5py.File(os.path.join(data_path,\"g_Ymodel_S1BNIR.hdf5\"), mode=\"a\") as f:\n",
    "    f[\"id_sar\"] = S1_grid_id               \n",
    "    f[\"id_rgb\"] = S2_grid_id                       \n",
    "    f[\"patch_sar_path\"] = S1_grid_path  \n",
    "    f[\"patch_rgb_path\"] = S2_grid_path\n",
    "\n",
    "print(\"Data exported in %s\"%(data_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a57566-1784-42da-b3cb-0cb18e77add0",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65dfa209-9aa1-4a86-8817-923fb93cb631",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "f = h5py.File('Images/big_raster/tr_BNIR_patch/tr_Ymodel_S1BNIR.hdf5', 'r')\n",
    "S1_patch_id = np.array(f[\"id_sar\"])\n",
    "S2_patch_id = np.array(f[\"id_rgb\"])\n",
    "S1_patch_lulc = np.array(f[\"lulc_sar\"])\n",
    "S2_patch_lulc = np.array(f[\"lulc_rgb\"])\n",
    "S1_patch_path = f[\"patch_sar_path\"]\n",
    "S2_patch_path = f[\"patch_rgb_path\"]\n",
    "                         \n",
    "f = h5py.File('Images/big_raster/g_BNIR_patch/g_Ymodel_S1BNIR.hdf5', 'r')\n",
    "S1_grid_id = np.array(f[\"id_sar\"])          \n",
    "S2_grid_id = np.array(f[\"id_rgb\"])                     \n",
    "S1_grid_path = f[\"patch_sar_path\"]\n",
    "S2_grid_path = f[\"patch_rgb_path\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33556ef2-5fd1-4259-86db-fe5740d30cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "stack = stackage(S1_grid_path)\n",
    "S1_stack_grid = normalise_01(stack)\n",
    "\n",
    "stack = stackage(S2_grid_path)\n",
    "S2_stack_grid = normalise_01(stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13eff9a0-c230-4e81-9320-4d5582c90b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "S1_patch_id = S1_patch_id.reshape(S1_patch_id.size,1)\n",
    "S2_patch_id = S2_patch_id.reshape(S2_patch_id.size,1)\n",
    "S1_patch_lulc = S1_patch_lulc.reshape(S1_patch_lulc.size,1)\n",
    "S2_patch_lulc = S2_patch_lulc.reshape(S2_patch_lulc.size,1)\n",
    "S1_grid_id = S1_grid_id.reshape(S1_grid_id.size,1)\n",
    "S2_grid_id = S2_grid_id.reshape(S2_grid_id.size,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f96a6d5-c4b0-4d84-a9b2-80276644d13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_id_train, patch_id_test, patch_lulc_train, patch_lulc_test = train_test_split(S1_patch_id,S1_patch_lulc, test_size=0.2, random_state=100)\n",
    "patch_id_train = reshape(patch_id_train)\n",
    "patch_id_test = reshape(patch_id_test)\n",
    "patch_lulc_train = reshape(patch_lulc_train)\n",
    "patch_lulc_test = reshape(patch_lulc_test)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a06f79-dba0-44bd-9533-aeab2ef24288",
   "metadata": {},
   "outputs": [],
   "source": [
    "circle(patch_lulc_train,patch_lulc_test,'Repartition des données entrainements et de tests')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9121b3b5-db00-44e4-974a-d6f1f592ef9a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "stack_train = assimilation(patch_id_train, patch_id_test, S2_patch_path)[0]\n",
    "stack_test = assimilation(patch_id_train, patch_id_test, S2_patch_path)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd20f822-9c6f-407e-a49c-b5b682882814",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "stack_sar_train = assimilation(patch_id_train, patch_id_test, S1_patch_path)[0]\n",
    "stack_sar_test = assimilation(patch_id_train, patch_id_test, S1_patch_path)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208b69de-5519-4fd6-9be7-2f4add4f29d4",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753900e4-fb05-413d-8d97-50949287cab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ymodel(left_input_shape,right_input_shape,batch_size,kernel_size,dropout,n_filters,stack_train,stack_test,stack_sar_train,stack_sar_test):\n",
    "    \n",
    "    class myCallback(tf.keras.callbacks.Callback):\n",
    "        def on_epoch_end(self, epoch, logs = {}):\n",
    "            if logs.get('val_accuracy') > 0.98:\n",
    "              print(\"\\n Enough accuracy\")\n",
    "              self.model.stop_training = True\n",
    "\n",
    "    maxcallbacks = myCallback()\n",
    "    callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, mode='min')\n",
    "    \n",
    "    myCallbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=50, mode='min'),\n",
    "                   tf.keras.callbacks.ModelCheckpoint(filepath='Images/checkpoint', save_weights_only=True, monitor='val_loss', mode='min', save_best_only=True),\n",
    "                   maxcallbacks, \n",
    "                   tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, min_lr=0.001) ]\n",
    "    \n",
    "    #Left branch\n",
    "    \n",
    "    left_inputs = Input(shape=left_input_shape)\n",
    "    x = left_inputs\n",
    "    filters = n_filters\n",
    "\n",
    "    x = RandomFlip(\"horizontal_and_vertical\")(x)\n",
    "    x = RandomRotation(0.2, fill_mode='reflect')(x)\n",
    "    #x = RandomTranslation(height_factor=0.05, width_factor=0.05, fill_mode='reflect')(x) \n",
    "\n",
    "    # 2 layer of Conv2D-Dropout-MaxPooling2D\n",
    "    for i in range(2):\n",
    "        x = Conv2D(filters=filters,\n",
    "                   kernel_size=kernel_size,\n",
    "                   padding='same',\n",
    "                   activation='relu')(x)\n",
    "        x = BatchNormalization(axis=-1)(x)\n",
    "        x = Dropout(dropout)(x)\n",
    "        filters *= 2\n",
    "        \n",
    "    x = MaxPooling2D()(x)\n",
    "    \n",
    "    \n",
    "    right_inputs = Input(shape=right_input_shape)\n",
    "    y = right_inputs\n",
    "    filters = n_filters\n",
    "\n",
    "\n",
    "\n",
    "    #Right branch\n",
    "\n",
    "    \n",
    "    y = RandomFlip(\"horizontal_and_vertical\")(y)\n",
    "    y = RandomRotation(0.2, fill_mode='reflect')(y)\n",
    "    #y = RandomTranslation(height_factor=0.05, width_factor=0.05, fill_mode='reflect')(y) \n",
    "    \n",
    "    # 2 layer of Conv2D-Dropout-MaxPooling2D\n",
    "    \n",
    "    for i in range(2):\n",
    "        y = Conv2D(filters=filters,\n",
    "                   kernel_size=kernel_size,\n",
    "                   padding='same',\n",
    "                   activation='relu',\n",
    "                   dilation_rate=2)(y)\n",
    "        y = BatchNormalization(axis=-1)(y)\n",
    "        y = Dropout(dropout)(y) \n",
    "        filters *= 2\n",
    "\n",
    "    y = MaxPooling2D()(y)\n",
    "       \n",
    "    \n",
    "    # merge left and right branches outputs\n",
    "    z = concatenate([x, y])\n",
    "    \n",
    "    # feature maps to vector before connecting to Dense \n",
    "    #num_labels = (0,1)\n",
    "    z = Flatten()(z)\n",
    "\n",
    "    z = Dense(512,activation='relu')(z)\n",
    "\n",
    "    outputs = Dense(1, activation=\"sigmoid\")(z)\n",
    "    \n",
    "    \n",
    "    model = Model([left_inputs, right_inputs], outputs)\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=Adam(learning_rate=1e-3),\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    history = model.fit([stack_train, stack_sar_train],\n",
    "          patch_lulc_train, \n",
    "          validation_data=([stack_test, stack_sar_test], patch_lulc_test),\n",
    "          epochs=100,\n",
    "          batch_size=batch_size, callbacks=[myCallbacks])\n",
    "    \n",
    "    score = model.evaluate([stack_test, stack_sar_test],\n",
    "                       patch_lulc_test,\n",
    "                       batch_size=batch_size,\n",
    "                       verbose=0)\n",
    "    \n",
    "    #model.load_weights('Images/checkpoint')\n",
    "    predict = model.predict([S2_stack_grid, S1_stack_grid])\n",
    "    \n",
    "    layers = model.layers\n",
    "    \n",
    "    tf.keras.utils.plot_model(\n",
    "                                model,\n",
    "                                to_file=\"Images/model.png\",\n",
    "                                show_shapes=True,\n",
    "                                show_dtype=True,\n",
    "                                show_layer_names=True,\n",
    "                                rankdir=\"TB\",\n",
    "                                expand_nested=False,\n",
    "                                dpi=96,\n",
    "                                layer_range=None,\n",
    "                                show_layer_activations=True,\n",
    "                                )\n",
    "\n",
    "    \n",
    "    print(score)\n",
    "    \n",
    "    return history, predict, layers, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dab24d2-59cc-4aa5-9a6d-25347d066f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4ed473-27ac-4203-be11-7551201baf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_input_shape = (10,10,4)\n",
    "right_input_shape = (10,10,3)\n",
    "batch_size = 32\n",
    "kernel_size = 3\n",
    "dropout = 0.5\n",
    "n_filters = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837020b4-5b75-46f1-8367-e809b87d0f38",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "output = ymodel(left_input_shape,right_input_shape,batch_size,kernel_size,dropout,n_filters,stack_train,stack_test,stack_sar_train,stack_sar_test)\n",
    "history = output[0]\n",
    "predict = output[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae05a4d9-b394-43e7-b837-9f0405bf651a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Training loss','Validation loss'], title = \"Legend\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46eca261-6234-4b65-87bc-54268b453f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"accuracy\"])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['Training accuracy','Validation accuracy'], title = \"Legend\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792dcb70-bc47-499e-9b96-c19441242e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(predict)\n",
    "plt.xlabel('Probability to be a slum')\n",
    "plt.ylabel('Number of entities')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2242fedd-5286-46d3-98e9-116459adf6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e140894e-2727-4a4d-974b-7213829e8c08",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = output[3]\n",
    "tf.keras.utils.plot_model(\n",
    "                                model,\n",
    "                                to_file=\"Images/Ymodel.png\",\n",
    "                                show_shapes=True,\n",
    "                                show_dtype=True,\n",
    "                                show_layer_names=True,\n",
    "                                rankdir=\"TB\",\n",
    "                                expand_nested=False,\n",
    "                                dpi=96,\n",
    "                                layer_range=None,\n",
    "                                show_layer_activations=True,\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82d5acd-b9bd-425f-89e0-f6ab88ff3459",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_csv(predict, S1_grid_id,'y_S1S2_BNIR_NEWSAMPLE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "6d89b0ab-65fd-4840-a23a-f04397a26343",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [121]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m probstab \u001b[38;5;241m=\u001b[39m mach_patch\u001b[38;5;241m.\u001b[39mGetField(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprobstab\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m mach_id \u001b[38;5;241m=\u001b[39m mach_patch\u001b[38;5;241m.\u001b[39mGetField(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[43mS1_grid_id\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m(mach_id)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m probstab \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m :\n\u001b[1;32m     12\u001b[0m     line \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mint\u001b[39m(mach_id),predict[index,\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m probstab]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'index'"
     ]
    }
   ],
   "source": [
    "from osgeo import ogr\n",
    "\n",
    "diff = []\n",
    "diff.append(['id','diff'])\n",
    "mach_res = ogr.Open('CLUSTERS/nairobi_grid100_clust_morpho_10pc_10cl.shp')\n",
    "mach_layer = mach_res.GetLayer()\n",
    "for mach_patch in mach_layer :\n",
    "    probstab = mach_patch.GetField('probstab')\n",
    "    mach_id = mach_patch.GetField('id')\n",
    "    index = S1_grid_id.index(mach_id)\n",
    "    if probstab != None :\n",
    "        line = [int(mach_id),predict[index,0] - probstab]\n",
    "        diff.append(line)\n",
    "    else :\n",
    "        line = [int(mach_id),None]\n",
    "        diff.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0615846e-4b88-42d5-ad27-9683efb24e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "name = 'machVSdeep2'\n",
    "\n",
    "with open(str(name)+'.csv','w') as file :\n",
    "    write = csv.writer(file)\n",
    "    write.writerows(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9005fb-7b70-407c-ac8d-2edf80507bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64893877-f494-4c38-b8f6-af9e73634adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, signal\n",
    "\n",
    "os.kill(os.getpid() , signal.SIGKILL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57da8b29-e2e3-4470-a0a9-d9693a346de9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
