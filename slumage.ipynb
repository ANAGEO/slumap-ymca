{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b38ea31-2295-4bc7-8e10-52ee0bff55f4",
   "metadata": {},
   "source": [
    "# **Slumage**\n",
    "**Jupyter notebook développé pour le pre-processing des images Sentinel-2 et Sentinel-1 pour les utiliser plus tard dans un modèle deep learning.** Ce notebook a été réalisé par Julien Govoorts dans le cardre d'un stage au sein de l'ANAGEO (IGEAT-ULB) pour le cours de STAG-F026 au sein du master en sciences et gestion de l'environnement à finalité sciences de l'environnement. Ce notebook utilise des données Sentinel-1 et 2 dans le but, dans un second temps, de cartographier (selon une probabilité) à partir d'un modèle deep learning les bidonvilles dans Nairobi (Kenya). \n",
    "\n",
    "Auteurs : Julien Govoorts et Taïs Grippa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d168c25c-4d9a-470a-9be4-e4b610612f95",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Import libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26092af-32cd-4ed2-85c3-a9a6d104e031",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from osgeo import gdal\n",
    "from osgeo import ogr\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e50841-a5f8-4f22-90d6-b0b6e38a485e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cad07b2-790e-4805-b169-ef9065be5474",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a35290-6407-472b-a07d-f09cb09ec395",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from cv2 import cvtColor, COLOR_BGR2RGB\n",
    "\t\n",
    "def Norma_Xpercentile(image_data, prct:int = 2, BGR2RGB=True):\n",
    "\t'''\n",
    "\tFunction that perform x percent histogram equalization of RGB images display\n",
    "\t'''\n",
    "\n",
    "\ta = np.ndarray(image_data.shape, dtype='float32')  \n",
    "\ta[:,:,0] = (image_data[:,:,0] - np.nanpercentile(image_data[:,:,0],prct))/(np.nanpercentile(image_data[:,:,0],100-prct) - np.nanpercentile(image_data[:,:,0],prct))\n",
    "\ta[:,:,1] = (image_data[:,:,1] - np.nanpercentile(image_data[:,:,1],prct))/(np.nanpercentile(image_data[:,:,1],100-prct) - np.nanpercentile(image_data[:,:,1],prct))\n",
    "\ta[:,:,2] = (image_data[:,:,2] - np.nanpercentile(image_data[:,:,2],prct))/(np.nanpercentile(image_data[:,:,2],100-prct) - np.nanpercentile(image_data[:,:,2],prct))\n",
    "\tif BGR2RGB: \n",
    "\t\ta = cvtColor(a, COLOR_BGR2RGB)\n",
    "\treturn a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3f93a9-93ce-4065-99dc-e406e7888113",
   "metadata": {},
   "source": [
    "## **Definition of functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5da0ce5-104c-4d6f-bed4-97d9bc1eecf7",
   "metadata": {},
   "source": [
    "### Function to extract coordinates from polygone shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e777c445-cc74-4f08-9f86-1b47ee5a34e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_coord(output_poly): #fonction pour extraire les coordonnées géographiques à partir d'un fichier json\n",
    "    \n",
    "    extract_json = json.loads(output_poly) #convertion de l'input dans le format json\n",
    "    coordinates = extract_json['coordinates'] #extraction des coordonnées\n",
    "    #extraction des coordonnées nécessaire selon la structure des données json\n",
    "    xmin = coordinates[0][0][0][0]\n",
    "    xmax = coordinates[0][0][1][0]\n",
    "    ymin = coordinates[0][0][0][1]\n",
    "    ymax = coordinates[0][0][2][1]\n",
    "    \n",
    "    return xmin,ymin,xmax,ymax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdca996-7ff5-40cc-9423-8b8ae8a2d093",
   "metadata": {},
   "source": [
    "### Function to merge multiple single band raster images to one raster multiplebands "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48a1fcd-6497-4d66-b4b4-f16814589eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "This function automatically merge multiple rasters with a SINGLE band to a raster with multiple bands like RGB.\n",
    "    Inputs :\n",
    "        listRGBNIR : This input MUST be a list with the different SINGLE band raster that must be merged.\n",
    "        output_dir : Is a string of the output directory. DO NOT FORGET to end the path with '/' Example : 'jgovoort/Images/S2/'\n",
    "    Outputs :\n",
    "        There is no output in Python. However, it will create a raster in GTiff format with all the merged bands in the indicate folder.\n",
    "        \n",
    "Contact : julien.govoorts@ulb.be\n",
    "\n",
    "jgovoort\n",
    "\n",
    "'''\n",
    "def merge_RGBNIR(listRGBNIR,output_dir):\n",
    "    gdal.BuildVRT(\"Images/RGBNIR.vrt\",listRGBNIR, separate=True) #rassemble les 3 rasters à bande unique en un seul raster sous un format vrt\n",
    "    output = output_dir+'merge_RGBNIR.tif'\n",
    "    gdal.Translate(output_dir+'merge_RGBNIR'+'.tif',\"Images/RGBNIR.vrt\", format='GTiff') #traduit le format vrt en format GTiff\n",
    "    \n",
    "merge_RGBNIR([\"Images/S2/S2_B04_2019_Q1_small.tif\",\"Images/S2/S2_B03_2019_Q1_small.tif\",\"Images/S2/S2_B02_2019_Q1_small.tif\",\"Images/S2/S2_B08_2019_Q1_small.tif\"],'Images/S2/')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247ba564-5848-4915-8cf7-342967722e36",
   "metadata": {},
   "source": [
    "### Function to normalise data\n",
    "Coming from [here](https://github.com/tgrippa/Partimap/blob/cd72ecd2f3e32794eb3e0ee70399de1811b5399e/Notebooks/Data_preprocessing.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8a233e-74de-4564-831c-e47ebfdf4c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code from https://github.com/tgrippa/Partimap/blob/cd72ecd2f3e32794eb3e0ee70399de1811b5399e/Notebooks/Data_preprocessing.ipynb\n",
    "def normalise_01(image_data):\n",
    "    image_data -= np.min(stack, axis=0)\n",
    "    image_data /= (np.max(stack, axis=0) - np.min(stack, axis=0))\n",
    "    return image_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d3edc7-d8e1-45c3-ac02-bc3fa98fdc2f",
   "metadata": {},
   "source": [
    "### Function to divide the global image to multiple images (patch) of 10x10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73585de0-d96d-4172-8087-827c13a22552",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def chunckage(folder_raster,filename_raster,format_rast,folder_vector, filename_vector, output_name, lulc_yes):\n",
    "    \n",
    "    rast = gdal.Open(folder_raster+filename_raster+format_rast) #ouverture du raster\n",
    "\n",
    "    vector = ogr.Open(folder_vector+filename_vector) #ouverture de la fichier gkpg\n",
    "\n",
    "    layer = vector.GetLayer() #extraction des couches dans le gkpg dans ce cas-ci il n'y en a qu'une seule (sinon spécifier)\n",
    "\n",
    "    print(layer.GetExtent()) #affiche de l'emprise de la couche vectorielle\n",
    "   \n",
    "    os.system('mkdir ./'+folder_raster+output_name+'_patch') #création automatique de la directory pour enregistrer les patchs\n",
    "    \n",
    "    #étape d'initialisation de la boucle\n",
    "    patch_id = [] #création des matrices\n",
    "    patch_lulc = []\n",
    "    listvrt = []\n",
    "\n",
    "    for feat in layer: #une boucle sur les polygones dans la couche vectorielle\n",
    "            polygone = feat.GetGeometryRef().ExportToJson() #extraire les données géométriques et exporter en format JSON\n",
    "            list_coord = extract_coord(polygone) #extraction des coordonnées des polygones\n",
    "            print(list_coord)\n",
    "            poly_id = int(feat.GetField(\"id\")) #extraire le id de l'object\n",
    "            \n",
    "            if lulc_yes == 1 : \n",
    "                lulc = int(feat.GetField(\"lulc\")) #extraire le lulc (si slum ou pas slum) de l'object\n",
    "                name_file= output_name+'_'+str(poly_id)+'_'+str(lulc)+'.tif' #création du nom du fichier de sortie\n",
    "                patch_lulc.append(lulc) #enregistrement dans une liste si le patch est un slum ou pas\n",
    "            elif lulc_yes == 0 :\n",
    "                name_file= output_name+'_'+str(poly_id)+'.tif' #création du nom du fichier de sortie\n",
    "            \n",
    "            filename = folder_raster+output_name+'_patch/'+str(name_file)\n",
    "            gdal.Warp(str(filename),rast, outputBounds = list_coord) #division en petite tuile correspondante aux polygone dans le shpfile\n",
    "            listvrt.append(filename) #enregistremet dans une liste le chemin des patchs\n",
    "            \n",
    "            patch_id.append(poly_id) #enregistrement dans une liste l'id du patch\n",
    "            \n",
    "\n",
    "    if lulc_yes == 1 :         \n",
    "        return listvrt, patch_id, patch_lulc\n",
    "    elif lulc_yes == 0 : \n",
    "        return listvrt, patch_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb0a722-fe3d-4e4d-9229-cf9455e5a8e4",
   "metadata": {},
   "source": [
    "## **Code**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67253968-eaf5-471d-84c6-317a46a964cf",
   "metadata": {},
   "source": [
    "### Creation of patchs and arraying its id and lulc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb589449-dcfb-4a0d-8119-e750480a161d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "merge_rbgnir_output = chunckage(\"Images/S2/\",\"merge_RGBNIR\",\".tif\",'Images/smaller/sample/','sample_small.gpkg','merge_RGBNIR',1)\n",
    "merge_rbgnir_list = merge_rbgnir_output[0] #inscrire dans une variable les path des patchs\n",
    "patch_id_merge_rgbnir = merge_rbgnir_output[1] #inscrire dans une variable les id des patchs\n",
    "patch_lulc_merge_rgbnir = merge_rbgnir_output[2] #inscrire dans une variable les lulc (slum ou pas) des patchs\n",
    "\n",
    "patch_id_merge_rgbnir = np.array(patch_id_merge_rgbnir) #convertion en array\n",
    "patch_lulc_merge_rgbnir = np.array(patch_lulc_merge_rgbnir)\n",
    "patch_id_merge_rgbnir = patch_id_merge_rgbnir.reshape(patch_id_merge_rgbnir.size, -1) #on \"transpose\" l'array pour inverser ses dimensions\n",
    "patch_lulc_merge_rgbnir = patch_lulc_merge_rgbnir.reshape(patch_lulc_merge_rgbnir.size, -1)\n",
    "\n",
    "print(patch_id_merge_rgbnir)\n",
    "print(patch_id_merge_rgbnir.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0edb16-3c6a-4ffa-879d-e6248e7d8078",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "grid_rbgnir_output = chunckage(\"Images/S2/\",\"merge_RGBNIR\",\".tif\",'Images/smaller/grid/','grid.gpkg','grid_RGBNIR',0)\n",
    "\n",
    "with h5py.File(os.path.join('Images/S2/grid_RGBNIR_patch/',\"grid.hdf5\"), mode=\"a\") as f:\n",
    "    f[\"raster\"] = grid_rbgnir_output[0]             #enregistrement des chemins des rasters\n",
    "    f[\"patch_id\"] = grid_rbgnir_output[1]             #enregistrement des chemins des rasters\n",
    "print(\"Data exported in %s\"%(data_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd0e5cd-10ff-4b31-9b3a-3a14eff0bb00",
   "metadata": {},
   "source": [
    "### Creation 4D Cube and normalisation of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3990bb33-93b6-4d8b-95f2-7159a30a68ec",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#code inspired from https://github.com/tgrippa/Partimap/blob/cd72ecd2f3e32794eb3e0ee70399de1811b5399e/Notebooks/Data_preprocessing.ipynb\n",
    "def stackage(list_raster):\n",
    "    stack = []\n",
    "    for patch in list_raster :\n",
    "        pat = gdal.Open(patch)\n",
    "        data_rast = pat.ReadAsArray().astype(np.float32)\n",
    "        data_rast = np.transpose(data_rast,(1,2,0))\n",
    "        stack.append(data_rast)\n",
    "\n",
    "    stack= np.array(stack)\n",
    "    print(stack)\n",
    "    print(stack.shape)\n",
    "    print(stack.dtype)\n",
    "   \n",
    "    return stack\n",
    "\n",
    "stack = stackage(merge_rbgnir_list)\n",
    "stack = normalise_01(stack)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edadd45c-ef9e-4414-890d-e5e1f415239b",
   "metadata": {},
   "source": [
    "### Split of train patchs and test patchs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1433f61c-ba35-4040-8258-6f461eb648f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#séparation des patchs pour l'entrainement et la validation. 80% vont servir d'entraiment et 20% de valisdation (d'ou 0.2) avec seed = 10 (random_state)\n",
    "patch_id_train, patch_id_test, patch_lulc_train, patch_lulc_test = train_test_split(patch_id_merge_rgbnir,patch_lulc_merge_rgbnir, test_size=0.2, random_state=10)\n",
    "\n",
    "print(patch_id_train)\n",
    "print(patch_lulc_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a445959-2a38-456d-bd30-161f1971a88d",
   "metadata": {},
   "source": [
    "### Save database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f175621-863d-4f41-8427-8b67453fc9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'Images/S2/merge_RGBNIR_patch/'\n",
    "with h5py.File(os.path.join(data_path,\"RGBNIR.hdf5\"), mode=\"a\") as f:\n",
    "    f[\"id_train\"] = patch_id_train               #enregistrement des id des patch qui vont servir à l'entrainement\n",
    "    f[\"id_test\"] = patch_id_test                 #enregistrement des id des patch qui vont servir à la validation\n",
    "    f[\"lulc_train\"] = patch_lulc_train           #enregistrement des lulc des patch qui vont servir à l'entrainement\n",
    "    f[\"lulc_test\"] = patch_lulc_test             #enregistrement des lulc des patch qui vont servir à la validation\n",
    "    f[\"patch_id\"] = patch_id_merge_rgbnir        #enregistrement de id des patchs\n",
    "    f[\"patch_lulc\"] = patch_lulc_merge_rgbnir    #enregistrement du lulc des patchs\n",
    "    f[\"raster\"] = merge_rbgnir_list              #enregistrement des chemins des rasters\n",
    "\n",
    "print(\"Data exported in %s\"%(data_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba75f0f-4b7f-4f0a-ae62-5f2aa65f3cac",
   "metadata": {},
   "source": [
    "## **Garbage test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebf78e7-fc55-4586-9c38-b6c1f46f7add",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 0.854165841\n",
    "print(\"my value = %.4f\"%f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c2145f-b7f8-4873-886d-d8827d7d58fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ab30c1-8e2c-412a-bac1-13c460fb5ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(stack[4,9,9,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ea5003-e398-4759-ae0d-f33019469217",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(Norma_Xpercentile(stack[0,:,:,]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d456b40-2f18-4005-a694-256e5c847e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(Norma_Xpercentile(stack[5,:,:,]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9154376a-4dd4-4b14-b2dd-e5883e3027ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47300aee-f439-4e3d-8f7d-c7156bece865",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ff8422-6ded-4b1c-81ca-679c7d826bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.removedirs('/home/jgovoort/patch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118e2816-1036-462b-b2b2-e4444ade70a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('./patch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e979a65-3eaf-48d1-a279-cbb42ef5daf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.join('home','jgovoort','patch') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6b66ec-a87b-4f60-9e97-4b424ef3b977",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/jgovoort/Images/Sentinel1.jpeg'\n",
    "if os.path.splitext(path)[1] != '.tiff' or os.path.splitext(path)[1] != '.tif':\n",
    "    sys.exit(\"NOT ALLOWED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353069e2-bc1d-412c-99a5-bfa22dc71351",
   "metadata": {},
   "outputs": [],
   "source": [
    "row = 'uygduhd,ddssdds,dsdsdsdsd,sddsszeez,dsdsd'\n",
    "'|'.join(row.split(','))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
